{"block_file": {"/home/src/custom/check_snowflake_connection.py:custom:python:home/src/custom/check snowflake connection": {"content": "if 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport snowflake.connector\n\n@custom\ndef transform_custom(*args, **kwargs):\n    \"\"\"\n    Conecta a Snowflake usando Secrets y hace un SELECT de verificaci\u00f3n.\n    Forzamos timeouts para evitar bloqueos largos.\n    \"\"\"\n    print(\"\ud83d\udd0d Intentando conectar con Snowflake (timeouts cortos)...\")\n\n    user = get_secret_value('SNOWFLAKE_USER')\n    password = get_secret_value('SNOWFLAKE_PASSWORD')\n    account = get_secret_value('SNOWFLAKE_ACCOUNT')  # p.ej. xpc24435.us-east-1\n    role = get_secret_value('SNOWFLAKE_ROLE')\n    warehouse = get_secret_value('SNOWFLAKE_WAREHOUSE')\n    database = get_secret_value('SNOWFLAKE_DB')\n    schema = get_secret_value('SNOWFLAKE_SCHEMA_BRONZE')\n\n    # Sugerencia: verifica visualmente que SNOWFLAKE_ACCOUNT sea EXACTAMENTE 'xpc24435.us-east-1'\n    print(f\"Cuenta: {account}, Usuario: {user}, DB: {database}, Schema: {schema}, WH: {warehouse}, Rol: {role}\")\n\n    conn = None\n    cur = None\n    try:\n        conn = snowflake.connector.connect(\n            user=user,\n            password=password,\n            account=account,              # sin '.snowflakecomputing.com'\n            role=role,\n            warehouse=warehouse,\n            database=database,\n            schema=schema,\n            login_timeout=20,             # segundos\n            network_timeout=20,           # segundos\n            client_session_keep_alive=False,\n            authenticator='snowflake',    # fuerza password auth est\u00e1ndar\n        )\n\n        cur = conn.cursor()\n        cur.execute(\"SELECT CURRENT_VERSION(), CURRENT_ROLE(), CURRENT_DATABASE(), CURRENT_SCHEMA();\")\n        v, r, d, s = cur.fetchone()\n\n        print(\"\u2705 Conexi\u00f3n exitosa a Snowflake!\")\n        print(f\"Versi\u00f3n: {v}\")\n        print(f\"Rol actual: {r}\")\n        print(f\"Base de datos: {d}\")\n        print(f\"Esquema: {s}\")\n        return {'version': v, 'role': r, 'database': d, 'schema': s}\n\n    except Exception as e:\n        print(\"\u274c Error al conectar a Snowflake:\")\n        print(repr(e))\n        # Si sigue colg\u00e1ndose en OCSP en tu red, como prueba temporal podr\u00edamos usar insecure_mode=True\n        # (no recomendado permanente). Av\u00edsame si el error persiste con timeout y lo activamos solo para la prueba.\n        raise\n    finally:\n        try:\n            if cur is not None:\n                cur.close()\n            if conn is not None:\n                conn.close()\n        except Exception:\n            pass\n\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None, 'El output es None; la conexi\u00f3n no devolvi\u00f3 datos.'\n", "file_path": "/home/src/custom/check_snowflake_connection.py", "language": "python", "type": "custom", "uuid": "check_snowflake_connection"}, "/home/src/custom/sf_setup_bronze.py:custom:python:home/src/custom/sf setup bronze": {"content": "# Mant\u00e9n este header del template\nif 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport snowflake.connector\n\n@custom\ndef transform_custom(*args, **kwargs):\n    \"\"\"\n    Crea FILE FORMAT Parquet y STAGE interno en DM_NYCTLC.BRONZE\n    \"\"\"\n    user = get_secret_value('SNOWFLAKE_USER')\n    password = get_secret_value('SNOWFLAKE_PASSWORD')\n    account = get_secret_value('SNOWFLAKE_ACCOUNT')\n    role = get_secret_value('SNOWFLAKE_ROLE')\n    warehouse = get_secret_value('SNOWFLAKE_WAREHOUSE')\n    database = get_secret_value('SNOWFLAKE_DB')\n    schema = get_secret_value('SNOWFLAKE_SCHEMA_BRONZE')  # BRONZE\n\n    print(f\"Usando {account} / {database}.{schema} con WH {warehouse} y rol {role}\")\n\n    sql_commands = [\n        # Asegurar contexto correcto\n        f\"USE ROLE {role}\",\n        f\"USE WAREHOUSE {warehouse}\",\n        f\"USE DATABASE {database}\",\n        f\"USE SCHEMA {schema}\",\n\n        # 1) FILE FORMAT Parquet (idempotente)\n        \"\"\"\n        CREATE OR REPLACE FILE FORMAT FF_PARQUET\n          TYPE = PARQUET\n        \"\"\",\n\n        # 2) STAGE interno para parquet de NYC TLC (idempotente)\n        #    Ah\u00ed subiremos archivos locales luego (v\u00eda Python).\n        \"\"\"\n        CREATE STAGE IF NOT EXISTS STG_NYCTLC_PARQUET\n          FILE_FORMAT = FF_PARQUET\n          COMMENT = 'Stage interno para archivos Parquet'\n        \"\"\",\n    ]\n\n    conn = None\n    cur = None\n    try:\n        conn = snowflake.connector.connect(\n            user=user,\n            password=password,\n            account=account,\n            role=role,\n            warehouse=warehouse,\n            database=database,\n            schema=schema,\n            login_timeout=20,\n            network_timeout=20,\n            client_session_keep_alive=False,\n            authenticator='snowflake',\n        )\n        cur = conn.cursor()\n        for i, cmd in enumerate(sql_commands, start=1):\n            print(f\"\u25b6 Ejecutando {i}/{len(sql_commands)}:\")\n            print(cmd.strip())\n            cur.execute(cmd)\n        print(\"\u2705 FILE FORMAT y STAGE creados/asegurados en BRONZE.\")\n\n        # Verificaci\u00f3n r\u00e1pida: listar el stage (vac\u00edo por ahora)\n        cur.execute(\"LIST @STG_NYCTLC_PARQUET\")\n        rows = cur.fetchall()\n        print(f\"Contenido actual del stage: {len(rows)} archivo(s).\")\n        return {\"stage_files\": len(rows)}\n\n    finally:\n        try:\n            if cur is not None:\n                cur.close()\n            if conn is not None:\n                conn.close()\n        except Exception:\n            pass\n\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None, \"No hubo output.\"\n    assert \"stage_files\" in output, \"Falta m\u00e9trica stage_files.\"\n", "file_path": "/home/src/custom/sf_setup_bronze.py", "language": "python", "type": "custom", "uuid": "sf_setup_bronze"}, "/home/src/custom/stage_one_parquet.py:custom:python:home/src/custom/stage one parquet": {"content": "# Mant\u00e9n el header del template\nif 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport snowflake.connector\nimport os\nimport pathlib\nfrom urllib.request import urlopen\nimport uuid  # <-- NUEVO: para run_id\n\n@custom\ndef transform_custom(*args, **kwargs):\n    \"\"\"\n    Descarga 1 parquet (service/year/month) a /home/src/data/nyctlc/<service>/\n    lo sube al stage @STG_NYCTLC_PARQUET en la ruta:\n      service=<service>/year=<yyyy>/month=<mm>/\n    Luego hace COPY INTO a BRONZE.<SERVICE>_PARQUET_RAW (con metadatos)\n    y lista el stage para verificar.\n    \"\"\"\n    # 1) Par\u00e1metros (puedes cambiar valores por defecto desde variables del pipeline)\n    if not kwargs.get('pipeline_runtime'):\n        kwargs['pipeline_runtime'] = {\n            'variables': {\n                'service': 'green',\n                'year': 2019,\n                'month': 1,\n            }\n        }\n\n    pr = kwargs.get('pipeline_runtime') or {}\n    service = (pr.get('variables', {}).get('service', 'yellow') or 'yellow').lower()\n    year = int(pr.get('variables', {}).get('year', 2019))\n    month = int(pr.get('variables', {}).get('month', 1))\n    mm = f'{month:02d}'\n    url = f'https://d37ci6vzurychx.cloudfront.net/trip-data/{service}_tripdata_{year}-{mm}.parquet'\n    stage_subdir = f\"service={service}/year={year}/month={mm}/\"\n    print(f\"\u27a1\ufe0f Vars: service={service} year={year} month={month} mm={mm}\")\n\n    # 2) Rutas locales\n    base_dir = pathlib.Path('/home/src/data/nyctlc') / service\n    base_dir.mkdir(parents=True, exist_ok=True)\n    local_path = base_dir / f'{service}_tripdata_{year}-{mm}.parquet'\n\n    print(f'\ud83d\udd3d Descargando: {url}')\n    with urlopen(url) as r, open(local_path, 'wb') as f:\n        chunk = r.read()\n        f.write(chunk)\n\n    size_mb = round(local_path.stat().st_size / (1024*1024), 2)\n    print(f'\u2705 Archivo descargado en {local_path} ({size_mb} MB)')\n\n    # 3) Conexi\u00f3n a Snowflake con Secrets (NO asumimos nombres: se leen de Secrets)\n    user = get_secret_value('SNOWFLAKE_USER')\n    password = get_secret_value('SNOWFLAKE_PASSWORD')\n    account = get_secret_value('SNOWFLAKE_ACCOUNT')\n    role = get_secret_value('SNOWFLAKE_ROLE')\n    warehouse = get_secret_value('SNOWFLAKE_WAREHOUSE')\n    database = get_secret_value('SNOWFLAKE_DB')               # ej: DM_NYCTLC\n    schema = get_secret_value('SNOWFLAKE_SCHEMA_BRONZE')      # debe ser BRONZE\n\n    conn = snowflake.connector.connect(\n        user=user,\n        password=password,\n        account=account,\n        role=role,\n        warehouse=warehouse,\n        database=database,\n        schema=schema,\n        login_timeout=60,\n        network_timeout=300,\n        client_session_keep_alive=True,\n        authenticator='snowflake',\n        insecure_mode=True,   \n    )\n\n    cur = conn.cursor()\n\n    try:\n        # Contexto expl\u00edcito\n        cur.execute(f\"USE ROLE {role}\")\n        cur.execute(f\"USE WAREHOUSE {warehouse}\")\n        cur.execute(f\"USE DATABASE {database}\")\n        cur.execute(f\"USE SCHEMA {schema}\")\n        cur.execute(\"ALTER SESSION SET STATEMENT_TIMEOUT_IN_SECONDS = 1200\")\n        cur.execute(\"ALTER SESSION SET LOCK_TIMEOUT = 3600\")\n\n        # 4) Subir al stage con PUT file://\n        print('\u2601\ufe0f  Subiendo al stage @STG_NYCTLC_PARQUET ...')\n        put_sql = (\n            f\"PUT file://{local_path} @STG_NYCTLC_PARQUET/{stage_subdir} \"\n            \"AUTO_COMPRESS=FALSE OVERWRITE=TRUE PARALLEL=1\"\n        )\n        print(put_sql)\n        cur.execute(put_sql)\n        _ = cur.fetchall()\n        print('\u2705 Subida completada.')\n\n        # 5) (NUEVO) Asegurar tabla destino en BRONZE (VARIANT + metadatos)\n        dest_table = 'YELLOW_PARQUET_RAW' if service == 'yellow' else 'GREEN_PARQUET_RAW'\n        create_sql = f\"\"\"\n        CREATE TABLE IF NOT EXISTS {schema}.{dest_table} (\n          v VARIANT,\n          src_file STRING,\n          ingest_ts TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),\n          src_service STRING,\n          src_year INT,\n          src_month INT,\n          run_id STRING\n        );\n        \"\"\"\n        print(f'\ud83d\udee0\ufe0f  Asegurando tabla destino: {schema}.{dest_table}')\n        cur.execute(create_sql)\n\n        # 6) (NUEVO) COPY INTO desde el stage a la tabla RAW con metadatos\n        run_id = f\"run_{uuid.uuid4().hex[:12]}\"\n        copy_sql = f\"\"\"\n        COPY INTO {schema}.{dest_table} (v, src_file, ingest_ts, src_service, src_year, src_month, run_id)\n        FROM (\n          SELECT\n            $1,\n            METADATA$FILENAME,\n            CURRENT_TIMESTAMP(),\n            %s,      -- src_service\n            %s,      -- src_year\n            %s,      -- src_month\n            %s       -- run_id\n          FROM @STG_NYCTLC_PARQUET/{stage_subdir}\n        )\n        FILE_FORMAT = (FORMAT_NAME = {schema}.FF_PARQUET)\n        ON_ERROR = 'ABORT_STATEMENT';\n        \"\"\"\n        print('\ud83d\udce5 Ejecutando COPY INTO (esto carga a BRONZE)...')\n        cur.execute(copy_sql, (service, year, month, run_id))\n        copy_result = cur.fetchall()\n        print('\u2705 COPY completado.')\n        # copy_result suele traer filas con info de carga; no siempre legible\n\n        # 7) Verificar contenido en el stage (opcional)\n        list_sql = f\"LIST @STG_NYCTLC_PARQUET/{stage_subdir}\"\n        cur.execute(list_sql)\n        files = cur.fetchall()\n        print(f'\ud83d\udce6 Archivos en {stage_subdir}: {len(files)}')\n        for row in files[:3]:\n            print(' -', row[0])\n\n        # 8) (Opcional) Conteo insertado por este run_id\n        cur.execute(f\"select count(*) from {schema}.{dest_table} where run_id = %s\", (run_id,))\n        inserted_count = cur.fetchone()[0]\n        print(f'\ud83e\uddee Filas insertadas en este run: {inserted_count}')\n\n        return {\n            \"local_file\": str(local_path),\n            \"size_mb\": size_mb,\n            \"stage_prefix\": stage_subdir,\n            \"files_in_stage\": len(files),\n            \"dest_table\": f\"{schema}.{dest_table}\",\n            \"run_id\": run_id,\n            \"rows_inserted\": inserted_count,\n        }\n\n    finally:\n        try:\n            cur.close()\n            conn.close()\n        except Exception:\n            pass\n\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None, \"No hubo output.\"\n    assert output.get(\"files_in_stage\", 0) >= 1, \"No se subi\u00f3 ning\u00fan archivo al stage.\"\n    assert output.get(\"rows_inserted\", 0) >= 1, \"No se insertaron filas en BRONZE.\"\n", "file_path": "/home/src/custom/stage_one_parquet.py", "language": "python", "type": "custom", "uuid": "stage_one_parquet"}, "/home/src/custom/zones_load.py:custom:python:home/src/custom/zones load": {"content": "if 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport snowflake.connector\nimport pathlib\nfrom urllib.request import urlopen\n\n@custom\ndef transform_custom(*args, **kwargs):\n    \"\"\"\n    Descarga Taxi Zones CSV y lo carga a Snowflake:\n      - Stage @STG_LOOKUPS (CSV)\n      - BRONZE.TAXI_ZONES_RAW (truncate + COPY)\n    Idempotente.\n    \"\"\"\n    # 1) Descarga CSV a /home/src/data/lookups/\n    base_dir = pathlib.Path('/home/src/data/lookups')\n    base_dir.mkdir(parents=True, exist_ok=True)\n    url = 'https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv'\n    local_path = base_dir / 'taxi_zone_lookup.csv'\n    print(f'\ud83d\udd3d Descargando: {url}')\n    with urlopen(url) as r, open(local_path, 'wb') as f:\n        f.write(r.read())\n    print(f'\u2705 Archivo en {local_path} ({round(local_path.stat().st_size/1024,1)} KB)')\n\n    # 2) Secrets Snowflake\n    user = get_secret_value('SNOWFLAKE_USER')\n    password = get_secret_value('SNOWFLAKE_PASSWORD')\n    account = get_secret_value('SNOWFLAKE_ACCOUNT')\n    role = get_secret_value('SNOWFLAKE_ROLE')\n    warehouse = get_secret_value('SNOWFLAKE_WAREHOUSE')\n    database = get_secret_value('SNOWFLAKE_DB')\n    schema_bronze = get_secret_value('SNOWFLAKE_SCHEMA_BRONZE')\n\n    # 3) Conexi\u00f3n\n    conn = snowflake.connector.connect(\n        user=user,\n        password=password,\n        account=account,\n        role=role,\n        warehouse=warehouse,\n        database=database,\n        schema=schema_bronze,\n        login_timeout=30,\n        network_timeout=30,\n        client_session_keep_alive=True,\n        authenticator='snowflake',\n        insecure_mode=True,   # \u2190 evita el fallo OCSP en PUT\n    )\n\n    cur = conn.cursor()\n\n    try:\n        cur.execute(f\"USE ROLE {role}\")\n        cur.execute(f\"USE WAREHOUSE {warehouse}\")\n        cur.execute(f\"USE DATABASE {database}\")\n        cur.execute(f\"USE SCHEMA {schema_bronze}\")\n\n        # 4) File format + stage (CSV)\n        print(\"\u25b6 Asegurando FILE FORMAT CSV y STAGE de lookups ...\")\n        cur.execute(\"\"\"\n            CREATE OR REPLACE FILE FORMAT FF_CSV_TAXI_ZONES\n              TYPE = CSV\n              FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n              SKIP_HEADER = 1\n              NULL_IF = ('', 'NULL');\n        \"\"\")\n        cur.execute(\"\"\"\n            CREATE STAGE IF NOT EXISTS STG_LOOKUPS\n              FILE_FORMAT = FF_CSV_TAXI_ZONES\n              COMMENT = 'Stage para archivos CSV de lookups';\n        \"\"\")\n\n        # 5) Subir CSV al stage\n        print(\"\u2601\ufe0f  Subiendo taxi_zone_lookup.csv al stage ...\")\n        put_sql = f\"PUT file://{local_path} @STG_LOOKUPS/lookups/ OVERWRITE=TRUE PARALLEL=1\"\n        print(put_sql)\n        cur.execute(put_sql)\n        print(\"\u2705 Subida completada.\")\n\n        # 6) Tabla BRONZE + carga idempotente\n        print(\"\u25b6 Creando tabla BRONZE.TAXI_ZONES_RAW (si no existe) ...\")\n        cur.execute(f\"\"\"\n            CREATE TABLE IF NOT EXISTS {database}.{schema_bronze}.TAXI_ZONES_RAW (\n                LocationID   INT,\n                Borough      STRING,\n                Zone         STRING,\n                service_zone STRING\n            );\n        \"\"\")\n        print(\"\ud83e\uddf9 TRUNCATE para recarga limpia ...\")\n        cur.execute(f\"TRUNCATE TABLE {database}.{schema_bronze}.TAXI_ZONES_RAW\")\n\n        print(\"\u25b6 COPY INTO desde @STG_LOOKUPS/lookups/ ...\")\n        cur.execute(f\"\"\"\n            COPY INTO {database}.{schema_bronze}.TAXI_ZONES_RAW\n            FROM @STG_LOOKUPS/lookups/\n            FILE_FORMAT = (FORMAT_NAME = FF_CSV_TAXI_ZONES)\n            ON_ERROR = 'ABORT_STATEMENT';\n        \"\"\")\n        print(\"\u2705 COPY ejecutado.\")\n\n        # 7) Verificaci\u00f3n\n        cur.execute(f\"SELECT COUNT(*) FROM {database}.{schema_bronze}.TAXI_ZONES_RAW\")\n        n = cur.fetchone()[0]\n        print(f\"\ud83d\udcca TAXI_ZONES_RAW filas: {n}\")\n        return {\"zones_rows\": n}\n\n    finally:\n        try:\n            cur.close()\n            conn.close()\n        except Exception:\n            pass\n\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None\n    assert output.get(\"zones_rows\", 0) > 0, \"No se cargaron filas a TAXI_ZONES_RAW\"\n", "file_path": "/home/src/custom/zones_load.py", "language": "python", "type": "custom", "uuid": "zones_load"}, "/home/src/custom/subprocess.py:custom:python:home/src/custom/subprocess": {"content": "if 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@custom\ndef transform_custom(*args, **kwargs):\n    \"\"\"\n    args: The output from any upstream parent blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your custom logic here\n\n    return {}\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "/home/src/custom/subprocess.py", "language": "python", "type": "custom", "uuid": "subprocess"}, "/home/src/custom/coverage_update.py:custom:python:home/src/custom/coverage update": {"content": "if 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport csv\nimport os\n\n@custom\ndef transform_custom(*args, **kwargs):\n    \"\"\"\n    Actualiza /home/src/docs/coverage.csv con la fila (service, year, month)\n    del \u00faltimo GOLD. Prefiere year/month de gold_row; si no existe, usa Variables.\n    Es idempotente: reemplaza la fila existente para ese mes/servicio.\n    \"\"\"\n    # --- Defaults desde Variables del pipeline ---\n    pr = kwargs.get('pipeline_runtime') or {}\n    vars_ = pr.get('variables', {})\n    service = vars_.get('service', 'yellow')\n    year = int(vars_.get('year', 2019))\n    month = int(vars_.get('month', 1))\n\n    # Acepta dict o lista o nada; si no es dict, usa {}\n    gold_metrics = args[0] if (args and isinstance(args[0], dict)) else {}\n    gold_row = gold_metrics.get('gold_row') if isinstance(gold_metrics, dict) else None\n\n    # \u26a0\ufe0f Preferir mes/a\u00f1o reales que calcul\u00f3 GOLD\n    if gold_row:\n        try:\n            year = int(gold_row[0])\n            month = int(gold_row[1])\n        except Exception:\n            pass\n\n    mm = f\"{month:02d}\"\n    print(f\"\u27a1\ufe0f Vars coverage (efectivas): service={service} year={year} month={month} mm={mm}\")\n\n    rows_bronze = int(gold_metrics.get('rows_bronze', 0))\n    rows_silver = int(gold_metrics.get('rows_silver', 0))\n    rows_gold   = int(gold_metrics.get('rows_gold', 0))\n\n    # --- Ruta / header ---\n    coverage_dir = '/home/src/docs'\n    coverage_path = f'{coverage_dir}/coverage.csv'\n    header = [\n        'service','year','month',\n        'bronze_loaded','silver_loaded','gold_loaded',\n        'rows_bronze','rows_silver','rows_gold'\n    ]\n    new_row = [\n        service, year, month,\n        'TRUE','TRUE','TRUE',\n        rows_bronze, rows_silver, rows_gold,\n    ]\n\n    os.makedirs(coverage_dir, exist_ok=True)\n\n    # --- Leer existente (si hay) ---\n    existing = []\n    if os.path.exists(coverage_path):\n        with open(coverage_path, 'r', newline='') as f:\n            r = csv.reader(f)\n            rows = list(r)\n            # Si el archivo ya tiene encabezado, conservarlo aparte\n            if rows and rows[0] and rows[0][0] == 'service':\n                existing = rows[1:]\n            else:\n                existing = rows\n\n    # --- Idempotencia: quitar cualquier fila previa del mismo service/year/month ---\n    filtered = []\n    for row in existing:\n        try:\n            s, y, m = row[0], int(row[1]), int(row[2])\n            if not (s == service and y == year and m == month):\n                filtered.append(row)\n        except Exception:\n            # Si hay filas corruptas, las mantenemos para no perder evidencia\n            filtered.append(row)\n\n    # --- Reescribir todo: header + filas previas + nueva fila ---\n    with open(coverage_path, 'w', newline='') as f:\n        w = csv.writer(f)\n        w.writerow(header)\n        w.writerows(filtered)\n        w.writerow(new_row)\n\n    print(f\"\u2705 Cobertura actualizada \u2192 {coverage_path}\")\n    print(\"Fila escrita:\", new_row)\n    return {'coverage_row': new_row}\n\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None\n", "file_path": "/home/src/custom/coverage_update.py", "language": "python", "type": "custom", "uuid": "coverage_update"}, "/home/src/custom/dbt.py:custom:python:home/src/custom/dbt": {"content": "if 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\n\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport os, subprocess\n\n@custom\ndef transform_custom(*args, **kwargs):\n    # 1) Cargar credenciales desde Secrets y ponerlas en el entorno del proceso hijo\n    env = os.environ.copy()\n    env['SNOWFLAKE_ACCOUNT']   = get_secret_value('SNOWFLAKE_ACCOUNT')\n    env['SNOWFLAKE_USER']      = get_secret_value('SNOWFLAKE_USER')\n    env['SNOWFLAKE_PASSWORD']  = get_secret_value('SNOWFLAKE_PASSWORD')\n    env['SNOWFLAKE_ROLE']      = get_secret_value('SNOWFLAKE_ROLE')\n    env['SNOWFLAKE_WAREHOUSE'] = get_secret_value('SNOWFLAKE_WAREHOUSE')\n    env['SNOWFLAKE_DB']        = get_secret_value('SNOWFLAKE_DB')\n    # Si usas schemas por env var en tu project: (opcionales)\n    # env['SNOWFLAKE_SCHEMA_BRONZE'] = get_secret_value('SNOWFLAKE_SCHEMA_BRONZE')\n    # env['SNOWFLAKE_SCHEMA_SILVER'] = get_secret_value('SNOWFLAKE_SCHEMA_SILVER')\n    # env['SNOWFLAKE_SCHEMA_GOLD']   = get_secret_value('SNOWFLAKE_SCHEMA_GOLD')\n\n    # Forzar a dbt a usar el profiles.yml de /home/src/dbt\n    env['DBT_PROFILES_DIR'] = '/home/src/dbt'\n\n    # 2) Comando dbt (proyecto en /home/src/dbt)\n    cmd = \"cd /home/src/dbt && dbt run --select silver_trips_unified fct_trips --full-refresh --project-dir /home/src/dbt --profiles-dir /home/src/dbt\"\n    print(f\"\u25b6 Ejecutando: {cmd}\")\n\n    # 3) Ejecutar y mostrar logs\n    result = subprocess.run(cmd, shell=True, capture_output=True, text=True, env=env)\n    print(result.stdout)\n    if result.returncode != 0:\n        print(result.stderr)\n        raise Exception(\"Error ejecutando dbt build\")\n\n    print(\"\u2705 DBT ejecutado correctamente.\")\n", "file_path": "/home/src/custom/dbt.py", "language": "python", "type": "custom", "uuid": "dbt"}, "/home/src/markdowns/notebook_documentation.md:markdown:markdown:home/src/markdowns/notebook documentation": {"content": "", "file_path": "/home/src/markdowns/notebook_documentation.md", "language": "markdown", "type": "markdown", "uuid": "notebook_documentation"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}